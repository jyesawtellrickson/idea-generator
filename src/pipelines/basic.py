from datetime import datetime
from langchain_ollama import ChatOllama
from langgraph.prebuilt import create_react_agent

from langgraph.checkpoint.memory import MemorySaver
from langchain_core.tools import tool
from langchain_core.messages import ToolMessage


from src.utils.api_helpers import (
    get_arxiv_papers,
    get_ieee_papers,
    get_pubmed_papers,
    get_springer_papers,
)

import json


def build_graph(args):
    @tool
    def arxiv_tool(keyword: str, num_results: int = 5) -> list[dict]:
        """
        Fetches the latest research papers from ArXiv.
        Important for generating research ideas.
        Covers many topics including computer science, physics, math, etc.
        """
        return get_arxiv_papers(keyword, num_results)

    @tool
    def springer_tool(keyword: str, num_results: int = 5) -> list[dict]:
        """
        Fetches the latest research papers from Springer Nature.
        Important for generating research ideas.
        Spring Nature specialises in science, technology, and medicine.
        """
        return get_springer_papers(keyword, num_results)

    @tool
    def ieee_tool(keyword: str, num_results: int = 5) -> list[dict]:
        """
        Fetches the latest research papers from IEEE Xplore.
        Important for generating research ideas.
        IEEE Xplore specialises in engineering and technology.
        """
        return get_ieee_papers(keyword, num_results)

    @tool
    def pubmed_tool(keyword: str, num_results: int = 5) -> list[dict]:
        """
        Fetches the latest research papers from PubMed.
        Important for generating research ideas.
        Pubmed specialises in medicine and biology.
        """
        return get_pubmed_papers(keyword, num_results)

    @tool
    def review_ideas(ideas: str) -> str:
        """
        Gives feedback on any generated ideas.
        Important when generating research ideas.
        :param ideas: The generated ideas, it should be the full ideas as currently understood.
        """
        system_prompt = (
            "You are reviewing the research ideas generated by an LLM."
            " Please provide feedback on the ideas. Be critical, no need"
            " to be overly positive. You should suggest improvements."
            " You can use the tools available to aid in your review."
        )
        model = ChatOllama(model=args.model)
        graph = create_react_agent(model, [arxiv_tool], state_modifier=system_prompt)
        inputs = {"messages": [("user", ideas)]}
        for s in graph.stream(inputs, stream_mode="values"):
            message = s["messages"][-1]
        # return f"Reviewing the ideas: {ideas}"
        return message.content

    @tool
    def score_ideas(ideas: str) -> str:
        """
        Gives scores on any generated ideas based on framework.
        Framework: Impact, Novelty, Feasibility.
        Important when generating research ideas.
        :param ideas: The generated ideas, it should be the full ideas as currently understood.
        """
        system_prompt = (
            "You are reviewing the research ideas generated by an LLM."
            " Please provide feedback on the ideas by utulising your"
            " Research Idea Grading Framework (RIGF)."
            " You should return a score for each Impact, Novelty and"
            " Feasibility on a scale of 1-5, clearly stating the reasons."
            " Be critical, don't be overly positive or wordy."
            " You can use the tools available to aid in your review."
        )
        model = ChatOllama(model=args.model)
        graph = create_react_agent(model, [arxiv_tool], state_modifier=system_prompt)
        inputs = {"messages": [("user", ideas)]}
        for s in graph.stream(inputs, stream_mode="values"):
            message = s["messages"][-1]
        # return f"Reviewing the ideas: {ideas}"
        return message.content

    # API tools
    tools = [arxiv_tool, ieee_tool, pubmed_tool, springer_tool]
    tools = [arxiv_tool]
    # Reviewer
    # tools += [review_ideas, score_ideas]

    model = ChatOllama(model=args.model)

    system_prompt = (
        "You are a good conversational chat agent that helps generate ideas."
        " When generating ideas, don't generate more than five."
        # " When you suggest ideas, share the pros and cons."
        # " Focus on impact of the work, novelty and feasibility."
        " If you're suggesting ideas, make sure to look at current"
        " research."
        " Do talk to and engage the user, don't just return lists."
        " IMPORTANT: always use your research APIs to get the latest research."
    )
    # system_prompt = "You are a helpful chat agent."
    # system_prompt = "Use the arxiv papers tool"

    graph = create_react_agent(
        model, tools, checkpointer=MemorySaver(), state_modifier=system_prompt, debug=True
    )
    return graph


def print_stream(graph, inputs, config):
    tool_results = []
    for s in graph.stream(inputs, config, stream_mode="values"):
        message = s["messages"][-1]
        if isinstance(message, ToolMessage):
            content = message.content
            if message.name == "arxiv_tool":
                try:
                    content = json.loads(content)
                    content = [c.get("title") for c in content]
                except:
                    content = content
            tool_results.append([message.name, content])
        if isinstance(message, tuple):
            print(message)
        else:
            message.pretty_print()
    # print([type(m) for m in s["messages"]])
    return message, tool_results


def run_basic(args):

    graph = build_graph(args)

    config = {"configurable": {"thread_id": "thread-1"}}

    init_message = (
        "\n\n---------------------------------------\n"
        "Welcome to the research idea generator!"
        " Please tell me about the area of research you're interested in to"
        " get started."
        "\n\nYou can quit by typing 'quit' or 'exit'."
    )
    print(init_message)
    tool_results = []
    init = True
    while True:
        user_input = input("\nUser: ")
        if init:
            inputs = {"messages": [("ai", init_message), ("user", user_input)]}
        else:
            inputs = {"messages": [("user", user_input)]}

        if user_input.lower() in ["quit", "exit", "q"]:
            print("Goodbye!")
            break

        tool_results = print_stream(graph, inputs, config)
        init = False

        if len(tool_results) > 0:
            print("Ran the tools!")
            print(tool_results)
