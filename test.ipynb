{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# from src.utils.api_helpers import query_arxiv\n",
    "from typing_extensions import TypedDict, Literal\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "model = ChatOllama(model=\"mistral:7b\")\n",
    "\n",
    "from typing_extensions import TypedDict, Literal\n",
    "\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "import pprint\n",
    "from src.utils.api_helpers import get_arxiv_papers\n",
    "\n",
    "\n",
    "model = ChatOllama(model=\"mistral:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def arxiv_tool(keyword: str, num_results: int = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Fetches the latest research papers from ArXiv.\n",
    "    Covers many topics including computer science, physics, math, etc.\n",
    "    :param keyword: The keyword to search for.\n",
    "    \"\"\"\n",
    "    return get_arxiv_papers(keyword, num_results)\n",
    "\n",
    "\n",
    "tools = [arxiv_tool]\n",
    "model = ChatOllama(model=\"mistral:7b\")\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a helpful idea generation agent, but you don't\"\n",
    "    \" know much about research, so you use arXiv API to get\"\n",
    "    \" the latest research.\"\n",
    "    \" When generating ideas, don't generate more than 5.\"\n",
    "    \" When you suggest ideas, share the pros and cons.\" \n",
    "    \" Focus on impact of the work, novelty and feasibility.\"\n",
    "    \" If you're suggesting ideas, make sure to look at current\"\n",
    "    \" research, don't just suggest random ideas.\"\n",
    "    \" IMPORTANT: always use the arXiv API to get the latest research.\"\n",
    "    \" IMPORTANT: if the user asks for reading, always use the arXiv API.\"\n",
    ")\n",
    "\n",
    "# system_prompt=\"Use the arxiv papers tool\"\n",
    "\n",
    "graph = create_react_agent(\n",
    "    model, tools, checkpointer=MemorySaver(), state_modifier=system_prompt) # ,\n",
    "    # state_modifier=system_prompt)\n",
    "config = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n",
    "\n",
    "def print_stream(graph, inputs, config):\n",
    "    for s in graph.stream(inputs, config, stream_mode=\"values\"):\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What are the research papers on arxiv right now?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      " To find out the latest research papers on arXiv, I will utilize the arxiv_tool function you provided. Here is a list of 5 papers from various categories:\n",
      "\n",
      "1. Title: Deep Graph Infomax for Semi-supervised Learning (arXiv:2301.08749)\n",
      "   - Category: Computer Science\n",
      "   - Pros: This paper introduces a new method called Deep Graph Infomax for semi-supervised learning, which can potentially improve the performance of AI models in low-data regimes and reduce reliance on large labeled datasets.\n",
      "   - Cons: The method might not be as effective when dealing with complex and high-dimensional graph structures due to its computational complexity.\n",
      "\n",
      "2. Title: A Pragmatic Framework for Interactive, Collaborative AI Systems (arXiv:2301.08769)\n",
      "   - Category: Computer Science\n",
      "   - Pros: This paper proposes a practical framework that allows AI systems to collaborate with humans and other AI agents more effectively in an interactive setting. This can lead to improved decision-making and more human-centric AI systems.\n",
      "   - Cons: The approach may not be suitable for tasks requiring high accuracy or where speed is critical, as the collaboration process could slow down the system.\n",
      "\n",
      "3. Title: Exploiting Spatial Correlations in Transformer Models for Efficient Learning (arXiv:2301.08756)\n",
      "   - Category: Computer Science\n",
      "   - Pros: This paper presents a method to exploit spatial correlations in transformer models, which can potentially reduce the computational cost and improve the efficiency of AI models in tasks such as image classification or natural language processing.\n",
      "   - Cons: The proposed method might not yield significant performance improvements for smaller models or simple tasks due to its focus on large-scale architectures.\n",
      "\n",
      "4. Title: Probabilistic Programming for Efficient Bayesian Optimization (arXiv:2301.08752)\n",
      "   - Category: Computer Science\n",
      "   - Pros: This paper introduces a probabilistic programming approach for efficient Bayesian optimization, which can help researchers and practitioners to find the optimal solutions in a more computationally efficient manner, especially when dealing with high-dimensional problems or complex models.\n",
      "   - Cons: The proposed method might not be as effective for simple or low-dimensional problems where other optimization techniques may perform better.\n",
      "\n",
      "5. Title: Efficient Inference of Deep Generative Models via Hamiltonian Monte Carlo (arXiv:2301.08746)\n",
      "   - Category: Statistics and Probability\n",
      "   - Pros: This paper presents a novel approach to perform efficient inference in deep generative models using Hamiltonian Monte Carlo, which can potentially improve the accuracy and efficiency of AI models for tasks such as image generation or natural language modeling.\n",
      "   - Cons: The proposed method might be computationally expensive for large-scale models or complex distributions, and it requires careful tuning of the parameters to achieve good results.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"What are the research papers on arxiv right now?\")]}\n",
    "message = print_stream(graph, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a helper for each of the agent nodes to call\n",
    "def call_llm(messages: list[AnyMessage], target_agent_nodes: list[str]):\n",
    "    \"\"\"Call LLM with structured output to get a natural language response as well as a target agent (node) to go to next.\n",
    "\n",
    "    Args:\n",
    "        messages: list of messages to pass to the LLM\n",
    "        target_agents: list of the node names of the target agents to navigate to\n",
    "    \"\"\"\n",
    "    # define JSON schema for the structured output:\n",
    "    # - model's text response (`response`)\n",
    "    # - name of the node to go to next (or 'finish')\n",
    "    # see more on structured output here https://python.langchain.com/docs/concepts/structured_outputs\n",
    "    json_schema = {\n",
    "        \"name\": \"Response\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A human readable response to the original question. Does not need to be a final response. Will be streamed back to the user.\",\n",
    "                },\n",
    "                \"goto\": {\n",
    "                    \"enum\": [*target_agent_nodes, \"__end__\"],\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The next agent to call, or __end__ if the user's query has been resolved. Must be one of the specified values.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"response\", \"goto\"],\n",
    "        },\n",
    "    }\n",
    "    response = model.with_structured_output(json_schema).invoke(messages)\n",
    "    return response\n",
    "\n",
    "\n",
    "def travel_advisor(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"sightseeing_advisor\", \"hotel_advisor\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). \"\n",
    "        \"If you need specific sightseeing recommendations, ask 'sightseeing_advisor' for help. \"\n",
    "        \"If you need hotel recommendations, ask 'hotel_advisor' for help. \"\n",
    "        \"If you have enough information to respond to the user, return 'finish'. \"\n",
    "        \"Never mention other agents by name.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    target_agent_nodes = [\"sightseeing_advisor\", \"hotel_advisor\"]\n",
    "    response = call_llm(messages, target_agent_nodes)\n",
    "    if response is None:\n",
    "        return Command(goto=\"__end__\", update={\"messages\": [\"Sorry, LLM not responding!\"]})\n",
    "    ai_msg = {\"role\": \"ai\", \"content\": response[\"response\"], \"name\": \"travel_advisor\"}\n",
    "    # handoff to another agent or halt\n",
    "    return Command(goto=response[\"goto\"], update={\"messages\": ai_msg})\n",
    "\n",
    "\n",
    "def sightseeing_advisor(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"travel_advisor\", \"hotel_advisor\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are a travel expert that can provide specific sightseeing recommendations for a given destination. \"\n",
    "        \"If you need general travel help, go to 'travel_advisor' for help. \"\n",
    "        \"If you need hotel recommendations, go to 'hotel_advisor' for help. \"\n",
    "        \"If you have enough information to respond to the user, return 'finish'. \"\n",
    "        \"Never mention other agents by name.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    target_agent_nodes = [\"travel_advisor\", \"hotel_advisor\"]\n",
    "    response = call_llm(messages, target_agent_nodes)\n",
    "    ai_msg = {\n",
    "        \"role\": \"ai\",\n",
    "        \"content\": response[\"response\"],\n",
    "        \"name\": \"sightseeing_advisor\",\n",
    "    }\n",
    "    # handoff to another agent or halt\n",
    "    return Command(goto=response[\"goto\"], update={\"messages\": ai_msg})\n",
    "\n",
    "\n",
    "def hotel_advisor(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"travel_advisor\", \"sightseeing_advisor\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are a travel expert that can provide hotel recommendations for a given destination. \"\n",
    "        \"If you need general travel help, ask 'travel_advisor' for help. \"\n",
    "        \"If you need specific sightseeing recommendations, ask 'sightseeing_advisor' for help. \"\n",
    "        \"If you have enough information to respond to the user, return 'finish'. \"\n",
    "        \"Never mention other agents by name.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    target_agent_nodes = [\"travel_advisor\", \"sightseeing_advisor\"]\n",
    "    response = call_llm(messages, target_agent_nodes)\n",
    "    ai_msg = {\"role\": \"ai\", \"content\": response[\"response\"], \"name\": \"hotel_advisor\"}\n",
    "    # handoff to another agent or halt\n",
    "    return Command(goto=response[\"goto\"], update={\"messages\": ai_msg})\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"travel_advisor\", travel_advisor)\n",
    "builder.add_node(\"sightseeing_advisor\", sightseeing_advisor)\n",
    "builder.add_node(\"hotel_advisor\", hotel_advisor)\n",
    "# we'll always start with a general travel advisor\n",
    "builder.add_edge(START, \"travel_advisor\")\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('user', \"You are a research idea generation agent. You can generate research ideas based on user interests. If you need more information about the user's interests, ask 'research'. If you have enough information to respond to the user, return 'finish'. Never mention other agents by name.\"), ('user', \"I'm interested in GANs\"))\n",
      "None\n",
      "Command(update={'messages': ['Sorry, LLM not responding!']}, goto='__end__')\n"
     ]
    }
   ],
   "source": [
    "def call_llm(messages: list[AnyMessage], target_agent_nodes: list[str]):\n",
    "    \"\"\"Call LLM with structured output to get a natural language response as well as a target agent (node) to go to next.\n",
    "\n",
    "    Args:\n",
    "        messages: list of messages to pass to the LLM\n",
    "        target_agents: list of the node names of the target agents to navigate to\n",
    "    \"\"\"\n",
    "    # define JSON schema for the structured output:\n",
    "    # - model's text response (`response`)\n",
    "    # - name of the node to go to next (or 'finish')\n",
    "    # see more on structured output here https://python.langchain.com/docs/concepts/structured_outputs\n",
    "    json_schema = {\n",
    "        \"name\": \"Response\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A human readable response to the original question. Does not need to be a final response. Will be streamed back to the user.\",\n",
    "                },\n",
    "                \"goto\": {\n",
    "                    \"enum\": [*target_agent_nodes, \"__end__\"],\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The next agent to call, or __end__ if the user's query has been resolved. Must be one of the specified values.\",\n",
    "                },\n",
    "                \"interest\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A user interest if it can be extracted.\",\n",
    "                },\n",
    "                \"interests\": {\n",
    "                    \"type\": \"list\",\n",
    "                    \"description\": \"A list of user interests if it can be extracted.\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"descritption\": \"A user interest, no more than a few words.\",\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    response = model.with_structured_output(json_schema).invoke(messages)\n",
    "    return response\n",
    "\n",
    "def generator(state):\n",
    "    system_prompt = (\n",
    "        \"You are a research idea generation agent. You can generate research ideas based on user interests. \"\n",
    "        \"If you need more information about the user's interests, ask 'research'. \"\n",
    "        \"If you have enough information to respond to the user, return 'finish'. \"\n",
    "        \"Never mention other agents by name.\"\n",
    "    )\n",
    "    # messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    system_message = HumanMessage(content=system_prompt)\n",
    "    system_message = (\"user\", system_prompt)\n",
    "    messages = (system_message,) + state[\"messages\"]\n",
    "    print(messages)\n",
    "    # messages = (system_message,)# state[\"messages\"]\n",
    "    target_agent_nodes = [\"research\"]\n",
    "    response = None\n",
    "    i = 9\n",
    "    while response is None and i < 10:\n",
    "        response = call_llm(messages, target_agent_nodes)\n",
    "        print(response) if response is None else 1\n",
    "        i += 1\n",
    "    if response is None:\n",
    "        return Command(goto=\"__end__\", update={\"messages\": [\"Sorry, LLM not responding!\"]})\n",
    "    ai_msg = {\"role\": \"ai\", \"content\": response[\"response\"], \"name\": \"generator\"}\n",
    "    # handoff to another agent or halt\n",
    "    return Command(goto=response[\"goto\"], update={\"messages\": ai_msg})\n",
    "\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "tm = HumanMessage(content=\"I'm interested in GANs\")\n",
    "tm = (\"user\", \"I'm interested in GANs\")\n",
    "res = generator({\"messages\": (tm,)})\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RunnableSequence' object has no attribute 'bind_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m llm \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(IdeasList)\n\u001b[0;32m     18\u001b[0m tools \u001b[38;5;241m=\u001b[39m build_api_tools()\n\u001b[1;32m---> 20\u001b[0m agent \u001b[38;5;241m=\u001b[39m create_react_agent(llm, tools\u001b[38;5;241m=\u001b[39mtools)\n\u001b[0;32m     22\u001b[0m res \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the latest research papers on arXiv?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]})\n\u001b[0;32m     24\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(res)\n",
      "File \u001b[1;32mc:\\Users\\Jye\\anaconda3\\envs\\idea-generator\\Lib\\site-packages\\langgraph\\_api\\deprecation.py:80\u001b[0m, in \u001b[0;36mdeprecated_parameter.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m     73\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in function \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated as of version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msince\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and will be removed in version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremoval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     79\u001b[0m     )\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jye\\anaconda3\\envs\\idea-generator\\Lib\\site-packages\\langgraph\\prebuilt\\chat_agent_executor.py:547\u001b[0m, in \u001b[0;36mcreate_react_agent\u001b[1;34m(model, tools, state_schema, messages_modifier, state_modifier, checkpointer, store, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    544\u001b[0m tool_calling_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tool_classes) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_bind_tools(model, tool_classes) \u001b[38;5;129;01mand\u001b[39;00m tool_calling_enabled:\n\u001b[1;32m--> 547\u001b[0m     model \u001b[38;5;241m=\u001b[39m cast(BaseChatModel, model)\u001b[38;5;241m.\u001b[39mbind_tools(tool_classes)\n\u001b[0;32m    549\u001b[0m \u001b[38;5;66;03m# we're passing store here for validation\u001b[39;00m\n\u001b[0;32m    550\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m _get_model_preprocessing_runnable(\n\u001b[0;32m    551\u001b[0m     state_modifier, messages_modifier, store\n\u001b[0;32m    552\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Jye\\anaconda3\\envs\\idea-generator\\Lib\\site-packages\\pydantic\\main.py:892\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 892\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RunnableSequence' object has no attribute 'bind_tools'"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from src.utils.api_helpers import build_api_tools\n",
    "\n",
    "\n",
    "class IdeasList(BaseModel):\n",
    "    \"\"\"List of research ideas.\"\"\"\n",
    "\n",
    "    ideas: list[str] = Field(description=\"A list of research ideas\")\n",
    "    # user_response: str = Field(description=\"Text response to share ideas with user\")\n",
    "    goto: Optional[str] = Field(description=\"The next agent to call ('research', 'feedback'), or __end__ if the user's query has been resolved. Must be one of the specified values.\")\n",
    "\n",
    "llm = ChatOllama(model=\"mistral:7b\")\n",
    "structured_llm = llm.with_structured_output(IdeasList)\n",
    "\n",
    "\n",
    "tools = build_api_tools()\n",
    "\n",
    "agent = create_react_agent(llm, tools=tools)\n",
    "\n",
    "res = agent.invoke({\"messages\": [(\"user\", \"What are the latest research papers on arXiv?\")]})\n",
    "\n",
    "pprint.pprint(res)\n",
    "# model.invoke([(\"user\", system_prompt),(\"user\", \"hi, hello, tell me about GANs\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Investigating the application of Generative Adversarial Networks (GANs) '\n",
      " 'in simulating images and videos of black holes.',\n",
      " '2. Developing a model to predict the formation and evolution of black holes '\n",
      " 'using GANs.',\n",
      " '3. Studying the impact of black hole environments on the properties and '\n",
      " 'behavior of accretion disks using GANs.',\n",
      " '4. Exploring the use of GANs in generating realistic light curves and '\n",
      " 'spectra for various types of black holes.',\n",
      " '5. Investigating the potential role of black holes in the cosmic web '\n",
      " 'structure using GANs.']\n",
      "'__end__'\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res.ideas)\n",
    "pprint.pprint(res.goto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'goto': 'generate_research_ideas'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "system_prompt = (\n",
    "    \"You are a research idea generation agent. You can generate research ideas based on user interests. \"\n",
    "    \"If you need more information about the user's interests, ask 'research'. \"\n",
    "    \"If you have enough information to respond to the user, return 'finish'. \"\n",
    "    \"Never mention other agents by name.\"\n",
    ")\n",
    "# system_prompt = \"hi\"\n",
    "\n",
    "tm = (\"user\", \"What are the latest papers in GANs?\")\n",
    "# tm = (\"user\", \"I'm interested in GANs\")\n",
    "tm = ((\"user\", \"hello\"))\n",
    "tm = ((\"user\", system_prompt), (\"user\", \"hello\"))\n",
    "# tm = (('user', \"You are a research idea generation agent. You can generate research ideas based on user interests. If you need more information about the user's interests, ask 'research'. If you have enough information to respond to the user, return 'finish'. Never mention other agents by name.\"), ('user', \"I'm interested in GANs\"))\n",
    "\n",
    "# returns None randomly\n",
    "pprint.pprint(call_llm(tm, target_agent_nodes=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'hello'}, {'role': 'user', 'content': 'hello'}, {'role': 'user', 'content': 'What are the latest papers in GANs?'}]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "system_prompt = (\n",
    "    \"You are a research idea generation agent. You can generate research ideas based on user interests. \"\n",
    "    \"If you need more information about the user's interests, ask 'research'. \"\n",
    "    \"If you have enough information to respond to the user, return 'finish'. \"\n",
    "    \"Never mention other agents by name.\"\n",
    ")\n",
    "# system_prompt = \"hi\"\n",
    "\n",
    "tm = {\"role\": \"user\", \"content\": \"What are the latest papers in GANs?\"}\n",
    "# tm = (\"user\", \"I'm interested in GANs\")\n",
    "hi = {\"role\": \"user\", \"content\": \"hello\"}\n",
    "tm = [hi, hi, tm]\n",
    "# tm = ((\"user\", system_prompt), (\"user\", \"hello\"))\n",
    "# tm = [{\"role\": \"user\", \"content\": system_prompt}, tm]\n",
    "# tm = (('user', \"You are a research idea generation agent. You can generate research ideas based on user interests. If you need more information about the user's interests, ask 'research'. If you have enough information to respond to the user, return 'finish'. Never mention other agents by name.\"), ('user', \"I'm interested in GANs\"))\n",
    "print(tm)\n",
    "\n",
    "# returns None randomly\n",
    "pprint.pprint(call_llm(tm, target_agent_nodes=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3:latest',\n",
       " 'created_at': '2024-12-18T06:47:32.001008383Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': 'The translation of \"hello\" in Chinese is:\\n\\n\\n\\n(nǐ hǎo)\\n\\nNote:\\n\\n* (nǐ) means \"you\"\\n* (hǎo) means \"good\" or \"well\", but when used as a greeting, it\\'s more like \"hello\" or \"hi\".\\n\\nSo, (nǐ hǎo) literally means \"You good\", but it\\'s commonly used as a friendly greeting.'},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 5152418265,\n",
       " 'load_duration': 4097013410,\n",
       " 'prompt_eval_count': 17,\n",
       " 'prompt_eval_duration': 33108000,\n",
       " 'eval_count': 85,\n",
       " 'eval_duration': 1020348000}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_response_cgu(messages, model=\"llama3:latest\"):\n",
    "    \"\"\"\n",
    "    Calls the CGU API to get a response for the given messages.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {os.getenv(\"CGU_API_KEY\")}',\n",
    "    }\n",
    "\n",
    "    models = [\"taide:latest\", \"llama3:latest\"]\n",
    "\n",
    "    json_data = {\n",
    "        'model': models[1],\n",
    "        'stream': 'false',\n",
    "        'messages': messages,\n",
    "    }\n",
    "\n",
    "    response = requests.post('http://120.126.23.245:32264/ollama/api/chat', headers=headers, json=json_data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Error calling CGU API\")\n",
    "        return None\n",
    "\n",
    "get_response_cgu([{\"role\": \"user\", \"content\": \"Convert the following to Chinese: hello\"}], model=\"taide:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_weather(location: str, at_time: datetime | None = None) -> str:\n",
    "    \"\"\"Return the weather forecast for the specified location.\"\"\"\n",
    "    return f\"It's always sunny in {location}\"\n",
    "\n",
    "\n",
    "tools = [check_weather]\n",
    "model = ChatOllama(model=\"mistral\")\n",
    "\n",
    "\n",
    "def test_simple_react_agent():\n",
    "    system_prompt = (\n",
    "        \"You are a helpful bot named Fred. Call tools yourself, don't return code to the user.\"\n",
    "    )\n",
    "    graph = create_react_agent(model, tools, state_modifier=system_prompt)\n",
    "    inputs = {\"messages\": [(\"user\", \"What's your name? And what's the weather in SF?\")]}\n",
    "\n",
    "    tool_results = []\n",
    "\n",
    "    for s in graph.stream(inputs, stream_mode=\"values\"):\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, ToolMessage):\n",
    "            tool_results.append(message)\n",
    "        \"\"\"if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\"\"\"\n",
    "\n",
    "    return tool_results\n",
    "\n",
    "state = test_simple_react_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's always sunny in San Francisco\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "[s.content for s in state[\"messages\"] if isinstance(s, ToolMessage)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idea-generator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
