{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# from src.utils.api_helpers import query_arxiv\n",
    "from typing_extensions import TypedDict, Literal\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "model = ChatOllama(model=\"mistral:7b\")\n",
    "\n",
    "from typing_extensions import TypedDict, Literal\n",
    "\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "import pprint\n",
    "from src.utils.api_helpers import get_arxiv_papers\n",
    "\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "model = ChatOllama(model=\"mistral:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def arxiv_tool(keyword: str, num_results: int = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Fetches the latest research papers from ArXiv.\n",
    "    Covers many topics including computer science, physics, math, etc.\n",
    "    :param keyword: The keyword to search for.\n",
    "    \"\"\"\n",
    "    return get_arxiv_papers(keyword, num_results)\n",
    "\n",
    "\n",
    "tools = [arxiv_tool]\n",
    "model = ChatOllama(model=\"mistral:7b\")\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a helpful idea generation agent, but you don't\"\n",
    "    \" know much about research, so you use arXiv API to get\"\n",
    "    \" the latest research.\"\n",
    "    \" When generating ideas, don't generate more than 5.\"\n",
    "    \" When you suggest ideas, share the pros and cons.\" \n",
    "    \" Focus on impact of the work, novelty and feasibility.\"\n",
    "    \" If you're suggesting ideas, make sure to look at current\"\n",
    "    \" research, don't just suggest random ideas.\"\n",
    "    \" IMPORTANT: always use the arXiv API to get the latest research.\"\n",
    "    \" IMPORTANT: if the user asks for reading, always use the arXiv API.\"\n",
    ")\n",
    "\n",
    "# system_prompt=\"Use the arxiv papers tool\"\n",
    "\n",
    "graph = create_react_agent(\n",
    "    model, tools, checkpointer=MemorySaver(), state_modifier=system_prompt) # ,\n",
    "    # state_modifier=system_prompt)\n",
    "config = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n",
    "\n",
    "def print_stream(graph, inputs, config):\n",
    "    for s in graph.stream(inputs, config, stream_mode=\"values\"):\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What are the research papers on arxiv right now?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      " To find out the latest research papers on arXiv, I will utilize the arxiv_tool function you provided. Here is a list of 5 papers from various categories:\n",
      "\n",
      "1. Title: Deep Graph Infomax for Semi-supervised Learning (arXiv:2301.08749)\n",
      "   - Category: Computer Science\n",
      "   - Pros: This paper introduces a new method called Deep Graph Infomax for semi-supervised learning, which can potentially improve the performance of AI models in low-data regimes and reduce reliance on large labeled datasets.\n",
      "   - Cons: The method might not be as effective when dealing with complex and high-dimensional graph structures due to its computational complexity.\n",
      "\n",
      "2. Title: A Pragmatic Framework for Interactive, Collaborative AI Systems (arXiv:2301.08769)\n",
      "   - Category: Computer Science\n",
      "   - Pros: This paper proposes a practical framework that allows AI systems to collaborate with humans and other AI agents more effectively in an interactive setting. This can lead to improved decision-making and more human-centric AI systems.\n",
      "   - Cons: The approach may not be suitable for tasks requiring high accuracy or where speed is critical, as the collaboration process could slow down the system.\n",
      "\n",
      "3. Title: Exploiting Spatial Correlations in Transformer Models for Efficient Learning (arXiv:2301.08756)\n",
      "   - Category: Computer Science\n",
      "   - Pros: This paper presents a method to exploit spatial correlations in transformer models, which can potentially reduce the computational cost and improve the efficiency of AI models in tasks such as image classification or natural language processing.\n",
      "   - Cons: The proposed method might not yield significant performance improvements for smaller models or simple tasks due to its focus on large-scale architectures.\n",
      "\n",
      "4. Title: Probabilistic Programming for Efficient Bayesian Optimization (arXiv:2301.08752)\n",
      "   - Category: Computer Science\n",
      "   - Pros: This paper introduces a probabilistic programming approach for efficient Bayesian optimization, which can help researchers and practitioners to find the optimal solutions in a more computationally efficient manner, especially when dealing with high-dimensional problems or complex models.\n",
      "   - Cons: The proposed method might not be as effective for simple or low-dimensional problems where other optimization techniques may perform better.\n",
      "\n",
      "5. Title: Efficient Inference of Deep Generative Models via Hamiltonian Monte Carlo (arXiv:2301.08746)\n",
      "   - Category: Statistics and Probability\n",
      "   - Pros: This paper presents a novel approach to perform efficient inference in deep generative models using Hamiltonian Monte Carlo, which can potentially improve the accuracy and efficiency of AI models for tasks such as image generation or natural language modeling.\n",
      "   - Cons: The proposed method might be computationally expensive for large-scale models or complex distributions, and it requires careful tuning of the parameters to achieve good results.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"What are the research papers on arxiv right now?\")]}\n",
    "message = print_stream(graph, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a helper for each of the agent nodes to call\n",
    "def call_llm(messages: list[AnyMessage], target_agent_nodes: list[str]):\n",
    "    \"\"\"Call LLM with structured output to get a natural language response as well as a target agent (node) to go to next.\n",
    "\n",
    "    Args:\n",
    "        messages: list of messages to pass to the LLM\n",
    "        target_agents: list of the node names of the target agents to navigate to\n",
    "    \"\"\"\n",
    "    # define JSON schema for the structured output:\n",
    "    # - model's text response (`response`)\n",
    "    # - name of the node to go to next (or 'finish')\n",
    "    # see more on structured output here https://python.langchain.com/docs/concepts/structured_outputs\n",
    "    json_schema = {\n",
    "        \"name\": \"Response\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A human readable response to the original question. Does not need to be a final response. Will be streamed back to the user.\",\n",
    "                },\n",
    "                \"goto\": {\n",
    "                    \"enum\": [*target_agent_nodes, \"__end__\"],\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The next agent to call, or __end__ if the user's query has been resolved. Must be one of the specified values.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"response\", \"goto\"],\n",
    "        },\n",
    "    }\n",
    "    response = model.with_structured_output(json_schema).invoke(messages)\n",
    "    return response\n",
    "\n",
    "\n",
    "def travel_advisor(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"sightseeing_advisor\", \"hotel_advisor\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). \"\n",
    "        \"If you need specific sightseeing recommendations, ask 'sightseeing_advisor' for help. \"\n",
    "        \"If you need hotel recommendations, ask 'hotel_advisor' for help. \"\n",
    "        \"If you have enough information to respond to the user, return 'finish'. \"\n",
    "        \"Never mention other agents by name.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    target_agent_nodes = [\"sightseeing_advisor\", \"hotel_advisor\"]\n",
    "    response = call_llm(messages, target_agent_nodes)\n",
    "    if response is None:\n",
    "        return Command(goto=\"__end__\", update={\"messages\": [\"Sorry, LLM not responding!\"]})\n",
    "    ai_msg = {\"role\": \"ai\", \"content\": response[\"response\"], \"name\": \"travel_advisor\"}\n",
    "    # handoff to another agent or halt\n",
    "    return Command(goto=response[\"goto\"], update={\"messages\": ai_msg})\n",
    "\n",
    "\n",
    "def sightseeing_advisor(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"travel_advisor\", \"hotel_advisor\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are a travel expert that can provide specific sightseeing recommendations for a given destination. \"\n",
    "        \"If you need general travel help, go to 'travel_advisor' for help. \"\n",
    "        \"If you need hotel recommendations, go to 'hotel_advisor' for help. \"\n",
    "        \"If you have enough information to respond to the user, return 'finish'. \"\n",
    "        \"Never mention other agents by name.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    target_agent_nodes = [\"travel_advisor\", \"hotel_advisor\"]\n",
    "    response = call_llm(messages, target_agent_nodes)\n",
    "    ai_msg = {\n",
    "        \"role\": \"ai\",\n",
    "        \"content\": response[\"response\"],\n",
    "        \"name\": \"sightseeing_advisor\",\n",
    "    }\n",
    "    # handoff to another agent or halt\n",
    "    return Command(goto=response[\"goto\"], update={\"messages\": ai_msg})\n",
    "\n",
    "\n",
    "def hotel_advisor(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"travel_advisor\", \"sightseeing_advisor\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are a travel expert that can provide hotel recommendations for a given destination. \"\n",
    "        \"If you need general travel help, ask 'travel_advisor' for help. \"\n",
    "        \"If you need specific sightseeing recommendations, ask 'sightseeing_advisor' for help. \"\n",
    "        \"If you have enough information to respond to the user, return 'finish'. \"\n",
    "        \"Never mention other agents by name.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    target_agent_nodes = [\"travel_advisor\", \"sightseeing_advisor\"]\n",
    "    response = call_llm(messages, target_agent_nodes)\n",
    "    ai_msg = {\"role\": \"ai\", \"content\": response[\"response\"], \"name\": \"hotel_advisor\"}\n",
    "    # handoff to another agent or halt\n",
    "    return Command(goto=response[\"goto\"], update={\"messages\": ai_msg})\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"travel_advisor\", travel_advisor)\n",
    "builder.add_node(\"sightseeing_advisor\", sightseeing_advisor)\n",
    "builder.add_node(\"hotel_advisor\", hotel_advisor)\n",
    "# we'll always start with a general travel advisor\n",
    "builder.add_edge(START, \"travel_advisor\")\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('user', \"You are a research idea generation agent. You can generate research ideas based on user interests. If you need more information about the user's interests, ask 'research'. If you have enough information to respond to the user, return 'finish'. Never mention other agents by name.\"), ('user', \"I'm interested in GANs\"))\n",
      "None\n",
      "Command(update={'messages': ['Sorry, LLM not responding!']}, goto='__end__')\n"
     ]
    }
   ],
   "source": [
    "def call_llm(messages: list[AnyMessage], target_agent_nodes: list[str]):\n",
    "    \"\"\"Call LLM with structured output to get a natural language response as well as a target agent (node) to go to next.\n",
    "\n",
    "    Args:\n",
    "        messages: list of messages to pass to the LLM\n",
    "        target_agents: list of the node names of the target agents to navigate to\n",
    "    \"\"\"\n",
    "    # define JSON schema for the structured output:\n",
    "    # - model's text response (`response`)\n",
    "    # - name of the node to go to next (or 'finish')\n",
    "    # see more on structured output here https://python.langchain.com/docs/concepts/structured_outputs\n",
    "    json_schema = {\n",
    "        \"name\": \"Response\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"response\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A human readable response to the original question. Does not need to be a final response. Will be streamed back to the user.\",\n",
    "                },\n",
    "                \"goto\": {\n",
    "                    \"enum\": [*target_agent_nodes, \"__end__\"],\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The next agent to call, or __end__ if the user's query has been resolved. Must be one of the specified values.\",\n",
    "                },\n",
    "                \"interest\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A user interest if it can be extracted.\",\n",
    "                },\n",
    "                \"interests\": {\n",
    "                    \"type\": \"list\",\n",
    "                    \"description\": \"A list of user interests if it can be extracted.\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"descritption\": \"A user interest, no more than a few words.\",\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    response = model.with_structured_output(json_schema).invoke(messages)\n",
    "    return response\n",
    "\n",
    "def generator(state):\n",
    "    system_prompt = (\n",
    "        \"You are a research idea generation agent. You can generate research ideas based on user interests. \"\n",
    "        \"If you need more information about the user's interests, ask 'research'. \"\n",
    "        \"If you have enough information to respond to the user, return 'finish'. \"\n",
    "        \"Never mention other agents by name.\"\n",
    "    )\n",
    "    # messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    system_message = HumanMessage(content=system_prompt)\n",
    "    system_message = (\"user\", system_prompt)\n",
    "    messages = (system_message,) + state[\"messages\"]\n",
    "    print(messages)\n",
    "    # messages = (system_message,)# state[\"messages\"]\n",
    "    target_agent_nodes = [\"research\"]\n",
    "    response = None\n",
    "    i = 9\n",
    "    while response is None and i < 10:\n",
    "        response = call_llm(messages, target_agent_nodes)\n",
    "        print(response) if response is None else 1\n",
    "        i += 1\n",
    "    if response is None:\n",
    "        return Command(goto=\"__end__\", update={\"messages\": [\"Sorry, LLM not responding!\"]})\n",
    "    ai_msg = {\"role\": \"ai\", \"content\": response[\"response\"], \"name\": \"generator\"}\n",
    "    # handoff to another agent or halt\n",
    "    return Command(goto=response[\"goto\"], update={\"messages\": ai_msg})\n",
    "\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "tm = HumanMessage(content=\"I'm interested in GANs\")\n",
    "tm = (\"user\", \"I'm interested in GANs\")\n",
    "res = generator({\"messages\": (tm,)})\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RunnableSequence' object has no attribute 'bind_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m llm \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(IdeasList)\n\u001b[0;32m     18\u001b[0m tools \u001b[38;5;241m=\u001b[39m build_api_tools()\n\u001b[1;32m---> 20\u001b[0m agent \u001b[38;5;241m=\u001b[39m create_react_agent(llm, tools\u001b[38;5;241m=\u001b[39mtools)\n\u001b[0;32m     22\u001b[0m res \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the latest research papers on arXiv?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]})\n\u001b[0;32m     24\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(res)\n",
      "File \u001b[1;32mc:\\Users\\Jye\\anaconda3\\envs\\idea-generator\\Lib\\site-packages\\langgraph\\_api\\deprecation.py:80\u001b[0m, in \u001b[0;36mdeprecated_parameter.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m     73\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in function \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated as of version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msince\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and will be removed in version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremoval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     79\u001b[0m     )\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jye\\anaconda3\\envs\\idea-generator\\Lib\\site-packages\\langgraph\\prebuilt\\chat_agent_executor.py:547\u001b[0m, in \u001b[0;36mcreate_react_agent\u001b[1;34m(model, tools, state_schema, messages_modifier, state_modifier, checkpointer, store, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    544\u001b[0m tool_calling_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tool_classes) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_bind_tools(model, tool_classes) \u001b[38;5;129;01mand\u001b[39;00m tool_calling_enabled:\n\u001b[1;32m--> 547\u001b[0m     model \u001b[38;5;241m=\u001b[39m cast(BaseChatModel, model)\u001b[38;5;241m.\u001b[39mbind_tools(tool_classes)\n\u001b[0;32m    549\u001b[0m \u001b[38;5;66;03m# we're passing store here for validation\u001b[39;00m\n\u001b[0;32m    550\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m _get_model_preprocessing_runnable(\n\u001b[0;32m    551\u001b[0m     state_modifier, messages_modifier, store\n\u001b[0;32m    552\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Jye\\anaconda3\\envs\\idea-generator\\Lib\\site-packages\\pydantic\\main.py:892\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 892\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RunnableSequence' object has no attribute 'bind_tools'"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from src.utils.api_helpers import build_api_tools\n",
    "\n",
    "\n",
    "class IdeasList(BaseModel):\n",
    "    \"\"\"List of research ideas.\"\"\"\n",
    "\n",
    "    ideas: list[str] = Field(description=\"A list of research ideas\")\n",
    "    # user_response: str = Field(description=\"Text response to share ideas with user\")\n",
    "    goto: Optional[str] = Field(description=\"The next agent to call ('research', 'feedback'), or __end__ if the user's query has been resolved. Must be one of the specified values.\")\n",
    "\n",
    "llm = ChatOllama(model=\"mistral:7b\")\n",
    "structured_llm = llm.with_structured_output(IdeasList)\n",
    "\n",
    "\n",
    "tools = build_api_tools()\n",
    "\n",
    "agent = create_react_agent(llm, tools=tools)\n",
    "\n",
    "res = agent.invoke({\"messages\": [(\"user\", \"What are the latest research papers on arXiv?\")]})\n",
    "\n",
    "pprint.pprint(res)\n",
    "# model.invoke([(\"user\", system_prompt),(\"user\", \"hi, hello, tell me about GANs\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Investigating the application of Generative Adversarial Networks (GANs) '\n",
      " 'in simulating images and videos of black holes.',\n",
      " '2. Developing a model to predict the formation and evolution of black holes '\n",
      " 'using GANs.',\n",
      " '3. Studying the impact of black hole environments on the properties and '\n",
      " 'behavior of accretion disks using GANs.',\n",
      " '4. Exploring the use of GANs in generating realistic light curves and '\n",
      " 'spectra for various types of black holes.',\n",
      " '5. Investigating the potential role of black holes in the cosmic web '\n",
      " 'structure using GANs.']\n",
      "'__end__'\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res.ideas)\n",
    "pprint.pprint(res.goto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'goto': 'generate_research_ideas'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "system_prompt = (\n",
    "    \"You are a research idea generation agent. You can generate research ideas based on user interests. \"\n",
    "    \"If you need more information about the user's interests, ask 'research'. \"\n",
    "    \"If you have enough information to respond to the user, return 'finish'. \"\n",
    "    \"Never mention other agents by name.\"\n",
    ")\n",
    "# system_prompt = \"hi\"\n",
    "\n",
    "tm = (\"user\", \"What are the latest papers in GANs?\")\n",
    "# tm = (\"user\", \"I'm interested in GANs\")\n",
    "tm = ((\"user\", \"hello\"))\n",
    "tm = ((\"user\", system_prompt), (\"user\", \"hello\"))\n",
    "# tm = (('user', \"You are a research idea generation agent. You can generate research ideas based on user interests. If you need more information about the user's interests, ask 'research'. If you have enough information to respond to the user, return 'finish'. Never mention other agents by name.\"), ('user', \"I'm interested in GANs\"))\n",
    "\n",
    "# returns None randomly\n",
    "pprint.pprint(call_llm(tm, target_agent_nodes=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'hello'}, {'role': 'user', 'content': 'hello'}, {'role': 'user', 'content': 'What are the latest papers in GANs?'}]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "system_prompt = (\n",
    "    \"You are a research idea generation agent. You can generate research ideas based on user interests. \"\n",
    "    \"If you need more information about the user's interests, ask 'research'. \"\n",
    "    \"If you have enough information to respond to the user, return 'finish'. \"\n",
    "    \"Never mention other agents by name.\"\n",
    ")\n",
    "# system_prompt = \"hi\"\n",
    "\n",
    "tm = {\"role\": \"user\", \"content\": \"What are the latest papers in GANs?\"}\n",
    "# tm = (\"user\", \"I'm interested in GANs\")\n",
    "hi = {\"role\": \"user\", \"content\": \"hello\"}\n",
    "tm = [hi, hi, tm]\n",
    "# tm = ((\"user\", system_prompt), (\"user\", \"hello\"))\n",
    "# tm = [{\"role\": \"user\", \"content\": system_prompt}, tm]\n",
    "# tm = (('user', \"You are a research idea generation agent. You can generate research ideas based on user interests. If you need more information about the user's interests, ask 'research'. If you have enough information to respond to the user, return 'finish'. Never mention other agents by name.\"), ('user', \"I'm interested in GANs\"))\n",
    "print(tm)\n",
    "\n",
    "# returns None randomly\n",
    "pprint.pprint(call_llm(tm, target_agent_nodes=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3:latest',\n",
       " 'created_at': '2024-12-18T06:47:32.001008383Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': 'The translation of \"hello\" in Chinese is:\\n\\n\\n\\n(nǐ hǎo)\\n\\nNote:\\n\\n* (nǐ) means \"you\"\\n* (hǎo) means \"good\" or \"well\", but when used as a greeting, it\\'s more like \"hello\" or \"hi\".\\n\\nSo, (nǐ hǎo) literally means \"You good\", but it\\'s commonly used as a friendly greeting.'},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 5152418265,\n",
       " 'load_duration': 4097013410,\n",
       " 'prompt_eval_count': 17,\n",
       " 'prompt_eval_duration': 33108000,\n",
       " 'eval_count': 85,\n",
       " 'eval_duration': 1020348000}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_response_cgu(messages, model=\"llama3:latest\"):\n",
    "    \"\"\"\n",
    "    Calls the CGU API to get a response for the given messages.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {os.getenv(\"CGU_API_KEY\")}',\n",
    "    }\n",
    "\n",
    "    models = [\"taide:latest\", \"llama3:latest\"]\n",
    "\n",
    "    json_data = {\n",
    "        'model': models[1],\n",
    "        'stream': 'false',\n",
    "        'messages': messages,\n",
    "    }\n",
    "\n",
    "    response = requests.post('http://120.126.23.245:32264/ollama/api/chat', headers=headers, json=json_data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Error calling CGU API\")\n",
    "        return None\n",
    "\n",
    "get_response_cgu([{\"role\": \"user\", \"content\": \"Convert the following to Chinese: hello\"}], model=\"taide:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolMessage(content=\"It's always sunny in San Francisco\", name='check_weather', id='f0f9789d-ee02-4379-bb4f-9840582b9e32', tool_call_id='f864b918-4c83-4934-9fc8-95ca728a9a2b')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_weather(location: str, at_time: datetime | None = None) -> str:\n",
    "    \"\"\"Return the weather forecast for the specified location.\"\"\"\n",
    "    return f\"It's always sunny in {location}\"\n",
    "\n",
    "\n",
    "tools = [check_weather]\n",
    "model = ChatOllama(model=\"mistral\")\n",
    "\n",
    "\n",
    "def test_simple_react_agent():\n",
    "    system_prompt = (\n",
    "        \"You are a helpful bot named Fred. Call tools yourself, don't return code to the user.\"\n",
    "    )\n",
    "    graph = create_react_agent(model, tools, state_modifier=system_prompt)\n",
    "    inputs = {\"messages\": [(\"user\", \"What's your name? And what's the weather in SF?\")]}\n",
    "\n",
    "    tool_results = []\n",
    "\n",
    "    for s in graph.stream(inputs, stream_mode=\"values\"):\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, ToolMessage):\n",
    "            tool_results.append(message)\n",
    "        \"\"\"if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\"\"\"\n",
    "\n",
    "    return tool_results\n",
    "\n",
    "test_simple_react_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"It's always sunny in San Francisco\", 'check_weather')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[(s.content, s.name) for s in state[\"messages\"] if isinstance(s, ToolMessage)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.api_helpers import build_api_tools, get_arxiv_papers\n",
    "\n",
    "tools = build_api_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='arxiv_tool', description='Fetches the latest research papers from ArXiv.\\nImportant for generating research ideas.\\nCovers many topics including computer science, physics, math, etc.', args_schema=<class 'langchain_core.utils.pydantic.arxiv_tool'>, func=<function build_api_tools.<locals>.arxiv_tool at 0x00000244CC23E0C0>)]\n"
     ]
    }
   ],
   "source": [
    "arxiv_tool = tools[:1]\n",
    "print(arxiv_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tool_call() missing 1 required keyword-only argument: 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AIMessage\n\u001b[0;32m      4\u001b[0m tool_node \u001b[38;5;241m=\u001b[39m ToolNode(arxiv_tool)\n\u001b[1;32m----> 6\u001b[0m message_with_single_tool_call \u001b[38;5;241m=\u001b[39m AIMessage(\n\u001b[0;32m      7\u001b[0m     content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     tool_calls\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      9\u001b[0m         {\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marxiv_tool\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGANs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_results\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m},\n\u001b[0;32m     12\u001b[0m         }\n\u001b[0;32m     13\u001b[0m     ],\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m tool_node\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [message_with_single_tool_call]})\n",
      "File \u001b[1;32mc:\\Users\\Jye\\anaconda3\\envs\\idea-generator\\Lib\\site-packages\\langchain_core\\messages\\ai.py:179\u001b[0m, in \u001b[0;36mAIMessage.__init__\u001b[1;34m(self, content, **kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m, content: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    172\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pass in content as positional arg.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m        content: The content of the message.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m        kwargs: Additional arguments to pass to the parent class.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(content\u001b[38;5;241m=\u001b[39mcontent, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jye\\anaconda3\\envs\\idea-generator\\Lib\\site-packages\\langchain_core\\messages\\base.py:76\u001b[0m, in \u001b[0;36mBaseMessage.__init__\u001b[1;34m(self, content, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m, content: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m     69\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pass in content as positional arg.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m        content: The string contents of the message.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m        kwargs: Additional fields to pass to the\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(content\u001b[38;5;241m=\u001b[39mcontent, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jye\\anaconda3\\envs\\idea-generator\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Jye\\anaconda3\\envs\\idea-generator\\Lib\\site-packages\\langchain_core\\messages\\ai.py:228\u001b[0m, in \u001b[0;36mAIMessage._backwards_compat_tool_calls\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    225\u001b[0m     updated: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tc \u001b[38;5;129;01min\u001b[39;00m tool_calls:\n\u001b[0;32m    227\u001b[0m         updated\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 228\u001b[0m             create_tool_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tc\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m    229\u001b[0m         )\n\u001b[0;32m    230\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m updated\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m invalid_tool_calls \u001b[38;5;241m:=\u001b[39m values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: tool_call() missing 1 required keyword-only argument: 'id'"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "tool_node = ToolNode(arxiv_tool)\n",
    "\n",
    "message_with_single_tool_call = AIMessage(\n",
    "    content=\"\",\n",
    "    tool_calls=[\n",
    "        {\n",
    "            \"name\": \"arxiv_tool\",\n",
    "            \"args\": {\"keyword\": \"GANs\", \"num_results\": 5},\n",
    "            \"id\": \"my_id\",\n",
    "            \"type\": \"tool_call\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "tool_node.invoke({\"messages\": [message_with_single_tool_call]})\n",
    "\n",
    "# arxiv_tool(\"GANs\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jye\\Documents\\code\\idea-generator\\src\\utils\\api_helpers.py:48: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  soup = BeautifulSoup(response.content, \"lxml\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'Generative Adversarial Networks and Adversarial Autoencoders: Tutorial\\n  and Survey',\n",
       "  'summary': '  This is a tutorial and survey paper on Generative Adversarial Network (GAN),\\nadversarial autoencoders, and their variants. We start with explaining\\nadversarial learning and the vanilla GAN. Then, we explain the conditional GAN\\nand DCGAN. The mode collapse problem is introduced and various methods,\\nincluding minibatch GAN, unrolled GAN, BourGAN, mixture GAN, D2GAN, and\\nWasserstein GAN, are introduced for resolving this problem. Then, maximum\\nlikelihood estimation in GAN are explained along with f-GAN, adversarial\\nvariational Bayes, and Bayesian GAN. Then, we cover feature matching in GAN,\\nInfoGAN, GRAN, LSGAN, energy-based GAN, CatGAN, MMD GAN, LapGAN, progressive\\nGAN, triple GAN, LAG, GMAN, AdaGAN, CoGAN, inverse GAN, BiGAN, ALI, SAGAN,\\nFew-shot GAN, SinGAN, and interpolation and evaluation of GAN. Then, we\\nintroduce some applications of GAN such as image-to-image translation\\n(including PatchGAN, CycleGAN, DeepFaceDrawing, simulated GAN, interactive\\nGAN), text-to-image translation (including StackGAN), and mixing image\\ncharacteristics (including FineGAN and MixNMatch). Finally, we explain the\\nautoencoders based on adversarial learning including adversarial autoencoder,\\nPixelGAN, and implicit autoencoder.\\n'},\n",
       " {'title': 'GAN You Do the GAN GAN?',\n",
       "  'summary': \"  Generative Adversarial Networks (GANs) have become a dominant class of\\ngenerative models. In recent years, GAN variants have yielded especially\\nimpressive results in the synthesis of a variety of forms of data. Examples\\ninclude compelling natural and artistic images, textures, musical sequences,\\nand 3D object files. However, one obvious synthesis candidate is missing. In\\nthis work, we answer one of deep learning's most pressing questions: GAN you do\\nthe GAN GAN? That is, is it possible to train a GAN to model a distribution of\\nGANs? We release the full source code for this project under the MIT license.\\n\"},\n",
       " {'title': 'Sequential training of GANs against GAN-classifiers reveals correlated\\n  \"knowledge gaps\" present among independently trained GAN instances',\n",
       "  'summary': '  Modern Generative Adversarial Networks (GANs) generate realistic images\\nremarkably well. Previous work has demonstrated the feasibility of\\n\"GAN-classifiers\" that are distinct from the co-trained discriminator, and\\noperate on images generated from a frozen GAN. That such classifiers work at\\nall affirms the existence of \"knowledge gaps\" (out-of-distribution artifacts\\nacross samples) present in GAN training. We iteratively train GAN-classifiers\\nand train GANs that \"fool\" the classifiers (in an attempt to fill the knowledge\\ngaps), and examine the effect on GAN training dynamics, output quality, and\\nGAN-classifier generalization. We investigate two settings, a small DCGAN\\narchitecture trained on low dimensional images (MNIST), and StyleGAN2, a SOTA\\nGAN architecture trained on high dimensional images (FFHQ). We find that the\\nDCGAN is unable to effectively fool a held-out GAN-classifier without\\ncompromising the output quality. However, StyleGAN2 can fool held-out\\nclassifiers with no change in output quality, and this effect persists over\\nmultiple rounds of GAN/classifier training which appears to reveal an ordering\\nover optima in the generator parameter space. Finally, we study different\\nclassifier architectures and show that the architecture of the GAN-classifier\\nhas a strong influence on the set of its learned artifacts.\\n'},\n",
       " {'title': 'Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities',\n",
       "  'summary': '  In this paper, we present the Lipschitz regularization theory and algorithms\\nfor a novel Loss-Sensitive Generative Adversarial Network (LS-GAN).\\nSpecifically, it trains a loss function to distinguish between real and fake\\nsamples by designated margins, while learning a generator alternately to\\nproduce realistic samples by minimizing their losses. The LS-GAN further\\nregularizes its loss function with a Lipschitz regularity condition on the\\ndensity of real data, yielding a regularized model that can better generalize\\nto produce new data from a reasonable number of training examples than the\\nclassic GAN. We will further present a Generalized LS-GAN (GLS-GAN) and show it\\ncontains a large family of regularized GAN models, including both LS-GAN and\\nWasserstein GAN, as its special cases. Compared with the other GAN models, we\\nwill conduct experiments to show both LS-GAN and GLS-GAN exhibit competitive\\nability in generating new images in terms of the Minimum Reconstruction Error\\n(MRE) assessed on a separate test set. We further extend the LS-GAN to a\\nconditional form for supervised and semi-supervised learning problems, and\\ndemonstrate its outstanding performance on image classification tasks.\\n'},\n",
       " {'title': 'Capsule GAN Using Capsule Network for Generator Architecture',\n",
       "  'summary': '  This paper presents Capsule GAN, a Generative adversarial network using\\nCapsule Network not only in the discriminator but also in the generator.\\nRecently, Generative adversarial networks (GANs) has been intensively studied.\\nHowever, generating images by GANs is difficult. Therefore, GANs sometimes\\ngenerate poor quality images. These GANs use convolutional neural networks\\n(CNNs). However, CNNs have the defect that the relational information between\\nfeatures of the image may be lost. Capsule Network, proposed by Hinton in 2017,\\novercomes the defect of CNNs. Capsule GAN reported previously uses Capsule\\nNetwork in the discriminator. However, instead of using Capsule Network,\\nCapsule GAN reported in previous studies uses CNNs in generator architecture\\nlike DCGAN. This paper introduces two approaches to use Capsule Network in the\\ngenerator. One is to use DigitCaps layer from the discriminator as the input to\\nthe generator. DigitCaps layer is the output layer of Capsule Network. It has\\nthe features of the input images of the discriminator. The other is to use the\\nreverse operation of recognition process in Capsule Network in the generator.\\nWe compare Capsule GAN proposed in this paper with conventional GAN using CNN\\nand Capsule GAN which uses Capsule Network in the discriminator only. The\\ndatasets are MNIST, Fashion-MNIST and color images. We show that Capsule GAN\\noutperforms the GAN using CNN and the GAN using Capsule Network in the\\ndiscriminator only. The architecture of Capsule GAN proposed in this paper is a\\nbasic architecture using Capsule Network. Therefore, we can apply the existing\\nimprovement techniques for GANs to Capsule GAN.\\n'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arxiv_tool(\"GANs\", 5)\n",
    "get_arxiv_papers(\"GANs\", 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idea-generator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
